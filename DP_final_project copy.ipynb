{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load = pd.read_csv('customer_experience_data.csv')\n",
    "\n",
    "# clean the data: fill the missing data with the mean\n",
    "def clean(df):\n",
    "    for col in df.columns:\n",
    "        mean_ = df[col].mean()\n",
    "        df[col] = df[col].fillna(mean_)\n",
    "    return df\n",
    "\n",
    "df_numerical = df_load[['Num_Interactions', 'Feedback_Score', 'Products_Purchased', 'Products_Viewed','Time_Spent_on_Site','Satisfaction_Score']].copy()\n",
    "df = clean(df_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training dataset and testing dataset\n",
    "train,test = train_test_split(df, train_size = 0.7, random_state=42)\n",
    "\n",
    "X_train = train[['Num_Interactions', 'Feedback_Score', 'Products_Purchased', 'Products_Viewed', 'Time_Spent_on_Site']].to_numpy()\n",
    "X_test = test[['Num_Interactions', 'Feedback_Score', 'Products_Purchased', 'Products_Viewed', 'Time_Spent_on_Site']].to_numpy()\n",
    "y_train = train['Satisfaction_Score'].to_numpy()\n",
    "y_test = test['Satisfaction_Score'].to_numpy()\n",
    "\n",
    "# normalize the data using z-score\n",
    "mean = X_train.mean(axis = 0)\n",
    "sd = X_train.std(axis = 0)\n",
    "X_train_normalized = (X_train - mean) / sd\n",
    "X_test_normalized = (X_test - mean) / sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 :linear regression model: build from scratch\n",
    "def linear_w_b(X, y, alpha, threshold):\n",
    "    # initialize the w and b\n",
    "    w = np.ones(X.shape[1])\n",
    "    b = 1\n",
    "\n",
    "    cost_previous = float('Inf')\n",
    "    improvement = threshold + 1\n",
    "    \n",
    "    while improvement > threshold:\n",
    "\n",
    "         #get the y_hat using the current w and b\n",
    "        y_hat = np.matmul(X, w)+ b\n",
    "        \n",
    "         # get the gradient of w and b\n",
    "        m = len(y)\n",
    "        X_T = np.transpose(X)\n",
    "        w_grad = (1 / m) * X_T @ (y_hat - y)\n",
    "        b_grad = (1 / m) * (np.sum(y_hat - y))\n",
    "        \n",
    "        w = w - alpha * w_grad\n",
    "        b = b - alpha * b_grad\n",
    "        \n",
    "        # compute the loss function\n",
    "        cost = (1 / (2 * m)) * (np.sum(np.square(y_hat - y)))\n",
    "        improvement = cost_previous - cost\n",
    "        \n",
    "        # prevent the divergence\n",
    "        if cost > cost_previous:\n",
    "            return 'model is diverging'\n",
    "        \n",
    "        cost_previous = cost\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model and get the test accracy\n",
    "w, b = linear_w_b(X_train_normalized, y_train, alpha=0.001, threshold=1e-20)\n",
    "prediction_linear = np.matmul(X_test_normalized, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: k-Nearest Neighbors for regression: build from scratch\n",
    "class KNN():\n",
    "    def __init__(self, X_train, y_train, k = 3):\n",
    "        self.k = k\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        \n",
    "        for row_test in X_test:\n",
    "            #Euclidean distances\n",
    "            distance = []\n",
    "            for row_train in self.X_train:\n",
    "                difference_sum = np.sum((row_train - row_test) ** 2)\n",
    "                distance_ = np.sqrt(difference_sum)\n",
    "                distance.append(distance_)\n",
    "            \n",
    "            distance = np.array(distance)\n",
    "            sorted_distance_index = np.argsort(distance)\n",
    "            k_index = sorted_distance_index[:self.k]\n",
    "            selected_y = self.y_train[k_index]\n",
    "            prediction = np.mean(selected_y)\n",
    "            predictions.append(prediction)\n",
    "                \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model and get the test accracy\n",
    "KNN_ = KNN(X_train_normalized, y_train, k=3)\n",
    "prediction_knn = KNN_.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Multivariant Polynomial Regression: use library\n",
    "polynomial_feature = PolynomialFeatures(degree=2)\n",
    "\n",
    "#transform the data to prepare for the fit\n",
    "X_train_transformed = polynomial_feature.fit_transform(X_train_normalized)\n",
    "X_test_transformed = polynomial_feature.transform(X_test_normalized)\n",
    "\n",
    "#fit the polynomial model\n",
    "polynomial = LinearRegression()\n",
    "polynomial.fit(X_train_transformed, y_train)\n",
    "\n",
    "#make prediction on test data\n",
    "prediction_polynomial = polynomial.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear regression rmse = 2.8658289792600367\n",
      "knn rmse = 3.3302207690137258\n",
      "polynomial rmse = 2.942011299575351\n"
     ]
    }
   ],
   "source": [
    "# determine the accuracy\n",
    "def accuracy(y_real, y_predict):\n",
    "    rmse = np.sqrt(mean_squared_error(y_real, y_predict))\n",
    "    message = f'rmse = {rmse}'\n",
    "    return message\n",
    "\n",
    "print(f'linear regression {accuracy(y_test, prediction_linear)}')\n",
    "print(f'knn {accuracy(y_test, prediction_knn)}')\n",
    "print(f'polynomial {accuracy(y_test, prediction_polynomial)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "1. Linear Regression is the best model for fitting the data.\n",
    "2. The difference between the performace of Multivariant Polynomial Regression and Linear Regression is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node class: determine which independent variable is most important\n",
    "class Node():\n",
    "    def __init__(self, feature_index = None, threshold = None, left = None, right = None, variance_reduction = None, value = None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.variance_reduction = variance_reduction\n",
    "        self.value = value\n",
    "\n",
    "# Tree class\n",
    "class DecisionTree():\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 2):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        \n",
    "    def build_tree(self, X, y, current_depth = 0):\n",
    "        nr_samples = X.shape[0]\n",
    "        nr_features = X.shape[1]\n",
    "        \n",
    "        # split further if the limits are not met\n",
    "        if nr_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            best_split = self.get_best_split(X, y, nr_samples, nr_features)\n",
    "            \n",
    "            # split further if the variance reduces\n",
    "            if best_split['variance_reduction'] > 0:\n",
    "                left_subtree = self.build_tree(best_split['subset_left_X'], best_split['subset_left_y'],current_depth + 1)\n",
    "                right_subtree = self.build_tree(best_split['subset_right_X'], best_split['subset_right_y'],current_depth + 1)\n",
    "                \n",
    "                node = Node(best_split['feature_index'], best_split['threshold'], left_subtree, right_subtree, best_split['variance_reduction'])\n",
    "                return node\n",
    "        \n",
    "        # in other cases: create a leaf node\n",
    "        else:\n",
    "            leaf_value = self.get_leaf_value(y)\n",
    "            node = Node(value = leaf_value)\n",
    "            return node\n",
    "        \n",
    "    def variance_reduction(self, parents, left_child, right_child):\n",
    "        weight_left = len(left_child) / len(parents)\n",
    "        weight_right = len(right_child) / len(parents)\n",
    "\n",
    "        variance_reduction_ = np.var(parents) - (weight_left * np.var(left_child) + weight_right * np.var(right_child))\n",
    "        return variance_reduction_\n",
    "\n",
    "    def get_leaf_value(self, y):\n",
    "        mean_y = np.mean(y)\n",
    "        return mean_y\n",
    "\n",
    "    def split_left_right(self, X, y, feature_index, threshold):\n",
    "        boolean_left = X[:, feature_index] <= threshold\n",
    "        boolean_right = X[:, feature_index] > threshold\n",
    "        \n",
    "        left_X = X[boolean_left]\n",
    "        left_y = y[boolean_left]\n",
    "        right_X = X[boolean_right]\n",
    "        right_y = y[boolean_right]\n",
    "\n",
    "        return left_X, left_y, right_X, right_y\n",
    "    \n",
    "    def get_best_split(self, X, y, nr_samples, nr_features):\n",
    "        best_split = {}\n",
    "        max_variance_reduction = -float('inf')\n",
    "        \n",
    "        # iterate over features to find best split\n",
    "        for i in range(nr_features):\n",
    "            feature_values = X[:, i]\n",
    "            potential_thresholds = np.unique(feature_values)\n",
    "            \n",
    "            # calculate variance reduction for every threshold in each feature\n",
    "            for threshold in potential_thresholds:\n",
    "                left_X, left_y, right_X, right_y = self.split_left_right(X, y, i, threshold)\n",
    "                if len(left_X) > 0 and len(right_X) > 0:\n",
    "                    current_variance_reduction = self.variance_reduction(y, left_y, right_y)\n",
    "                    \n",
    "                    # update the best split based on variance reduction\n",
    "                    if current_variance_reduction > max_variance_reduction:\n",
    "                        best_split['variance_reduction'] = current_variance_reduction\n",
    "                        best_split['feature_index'] = i\n",
    "                        best_split['threshold'] = threshold\n",
    "                        best_split['subset_left_X'] = left_X\n",
    "                        best_split['subset_left_y'] = left_y\n",
    "                        best_split['subset_right_X'] = right_X\n",
    "                        best_split['subset_right_y'] = right_y\n",
    "                        \n",
    "                        max_variance_reduction = current_variance_reduction\n",
    "            \n",
    "        return best_split\n",
    "    \n",
    "    def print_tree(self, tree = None, indent = ' '):\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(indent + \"X_\" + \n",
    "                  str(tree.feature_index) \n",
    "                  + \" <= \" + str(tree.threshold) \n",
    "                  + \" (var_red: \" \n",
    "                  + str(tree.variance_reduction) \n",
    "                  + \")\")\n",
    "            \n",
    "            print(f\"{indent}    Left:\")\n",
    "            self.print_tree(tree.left, indent + '       ')\n",
    "            print(f\"{indent}    Right:\")\n",
    "            self.print_tree(tree.right, indent + '       ')\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "        \n",
    "    def get_y_hat(self, single_x, tree):\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        feature_value = single_x[tree.feature_index]\n",
    "        \n",
    "        if feature_value <= tree.threshold:\n",
    "            y_hat = self.get_y_hat(single_x, tree.left)\n",
    "            return y_hat\n",
    "        else: \n",
    "            y_hat = self.get_y_hat(single_x, tree.right)\n",
    "            return y_hat\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = []\n",
    "        for single_x in X:\n",
    "            prediction.append(self.get_y_hat(single_x, self.root))\n",
    "        \n",
    "        return prediction\n",
    "        \n",
    "    def get_feature_importance(self):\n",
    "        importance = {}\n",
    "        \n",
    "        def variance_accelerator(node):\n",
    "            if node is None or node.value is not None:  \n",
    "                return\n",
    "            \n",
    "            # accumulate the variance reduction for the feature\n",
    "            feature_index = node.feature_index\n",
    "            if feature_index in importance:\n",
    "                importance[feature_index] += node.variance_reduction\n",
    "            else:\n",
    "                importance[feature_index] = node.variance_reduction\n",
    "                \n",
    "            #go to each of the left and right branches to visit every node\n",
    "            variance_accelerator(node.left)\n",
    "            variance_accelerator(node.right)\n",
    "        \n",
    "        #call from the beginning of the tree\n",
    "        variance_accelerator(self.root)\n",
    "        \n",
    "        total = sum(importance.values())\n",
    "        for f in importance:\n",
    "            importance[f] = importance[f]/total\n",
    "        return importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.103729187548726,\n",
       " 3: 0.2061139724114159,\n",
       " 4: 0.21552791102752097,\n",
       " 2: 0.22849904912264762,\n",
       " 1: 0.24612987988968943}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the model\n",
    "decision_tree = DecisionTree(min_samples_split = 4, max_depth = 6)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "# decision_tree.print_tree()\n",
    "\n",
    "decision_tree.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.457866745968065\n",
      "3.316548895830722\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "predict_train = decision_tree.predict(X_train)\n",
    "print(np.sqrt(mean_squared_error(y_train, predict_train)))\n",
    "\n",
    "predict_test = decision_tree.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, predict_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: \n",
    "1. The model fit is good, without big overfitting\n",
    "2. The most important variable to predict Satisfaction Score: 'Feedback_Score'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
